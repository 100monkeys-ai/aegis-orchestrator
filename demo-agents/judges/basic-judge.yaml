# Copyright (c) 2026 100monkeys.ai
# SPDX-License-Identifier: AGPL-3.0

apiVersion: 100monkeys.ai/v1
kind: AgentManifest
metadata:
  name: basic-judge
  version: "1.0.0"
  description: |
    A foundational judge agent that validates code execution results.
    Scores outputs on a 0.0-1.0 gradient scale based on:
    - Correctness: Does it solve the problem?
    - Completeness: Are all requirements met?
    - Quality: Is the output well-formed?
    
    This is the first implementation of Agent-as-Judge pattern,
    replacing hardcoded validation logic with declarative agent execution.
  
  labels:
    role: judge
    category: validation
    cognitive-function: evaluation
  
  annotations:
    examples: |
      Input: {"code": "print('hello')", "expected": "hello"}
      Output: {"score": 0.95, "reasoning": "Correct output, minor style issue"}

spec:
  # Runtime configuration
  runtime:
    language: python
    version: "3.11"
    isolation: firecracker
    timeout: 30s
  
  # Judge task definition
  task:
    instruction: |
      You are a Judge Agent responsible for evaluating code execution results.
      Analyze execution output and assign a validation score from 0.0 to 1.0.
      
      Scoring Rubric:
      - 1.0 (Perfect): Code executes successfully, output exactly matches requirements
      - 0.8-0.9 (Excellent): Functionally correct with minor style issues
      - 0.6-0.7 (Good): Core functionality works, some edge cases not handled
      - 0.4-0.5 (Fair): Partial functionality, significant bugs
      - 0.2-0.3 (Poor): Major errors or crashes
      - 0.0-0.1 (Failed): Complete failure, no meaningful output
      
      Return JSON: {"score": 0.0-1.0, "confidence": 0.0-1.0, "reasoning": "...", "suggestions": [...], "verdict": "pass|refine|fail"}
    
    prompt_template: |
      {{instruction}}
      
      ## Execution Result to Evaluate:
      {{input}}
      
      Judge:
  
  # Execution configuration
  execution:
    mode: "one-shot"  # Judges run once, no refinement
    max_iterations: 1
  
  # Security policy
  security:
    network:
      mode: "none"  # Judges don't need network access
    
    resources:
      cpu: 1000        # 1.0 CPU core
      memory: "512Mi"
      timeout: "30s"
  
  # Validation configuration
  validation:
    type: json_schema
    schema:
      type: object
      required: [score, confidence, reasoning, verdict]
      properties:
        score:
          type: number
          minimum: 0.0
          maximum: 1.0
          description: "Validation score (0.0 = failed, 1.0 = perfect)"
        
        confidence:
          type: number
          minimum: 0.0
          maximum: 1.0
          description: "Judge's confidence in the score (0.0 = uncertain, 1.0 = certain)"
        
        reasoning:
          type: string
          minLength: 20
          description: "Detailed explanation of the score"
        
        suggestions:
          type: array
          items:
            type: string
          description: "Actionable improvement suggestions"
        
        verdict:
          type: string
          enum: [pass, refine, fail]
          description: "Final verdict (pass >= 0.7, refine < 0.7 and > 0.3, fail <= 0.3)"
    
    error_handling:
      on_validation_failure: return_error
      on_timeout: return_partial
  
  # Environment variables for judge context
  environment:
    JUDGE_MODE: "standard"
    MIN_PASS_SCORE: "0.70"
    MAX_REFINEMENT_ITERATIONS: "10"
